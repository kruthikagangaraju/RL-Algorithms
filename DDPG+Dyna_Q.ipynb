{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYGgbiUqXBGQDD9hmyEygW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruthikagangaraju/RL-Algorithms/blob/main/DDPG%2BDyna_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install mujoco\n",
        "!pip install gymnasium[mujoco]\n",
        "!pip install gymnasium[atari]\n",
        "!pip install tensorboard\n",
        "!pip install packaging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BSDH-HuMI5t4",
        "outputId": "1375afb7-25ce-4263-f96e-9736befa6f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (910 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.0)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349140 sha256=ef3dabb07e3e62933f5c3839236e3385af6382efe19fbf6c9ef40b486ed248b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.10/dist-packages (3.2.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.7.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.4.4)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.20.1)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.3.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.34.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (2.7.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.20.1)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import gymnasium as gym\n",
        "import os, shutil"
      ],
      "metadata": {
        "id": "MurwTAnvHo6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, net_width, maxaction):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, net_width)\n",
        "        self.l2 = nn.Linear(net_width, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.maxaction = maxaction\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = torch.relu(self.l1(state))\n",
        "        a = torch.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.maxaction\n",
        "        return a"
      ],
      "metadata": {
        "id": "Nhvs0UgnLPuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Q_Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, net_width):\n",
        "        super(Q_Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, net_width)\n",
        "        self.l2 = nn.Linear(net_width, 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q = F.relu(self.l1(sa))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        return q"
      ],
      "metadata": {
        "id": "N21Vo4ztLTiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(env, agent, turns = 3):\n",
        "    total_scores = 0\n",
        "    for j in range(turns):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Take deterministic actions at test time\n",
        "            a = agent.select_action(s, deterministic=True)\n",
        "            s_next, r, dw, tr, info = env.step(a)\n",
        "            done = (dw or tr)\n",
        "\n",
        "            total_scores += r\n",
        "            s = s_next\n",
        "    return int(total_scores/turns)\n",
        "\n",
        "\n",
        "#Just ignore this function~\n",
        "def str2bool(v):\n",
        "    '''transfer str to bool for argparse'''\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in ('yes', 'True','true','TRUE', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'False','false','FALSE', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')"
      ],
      "metadata": {
        "id": "tliSRfUlLWtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG_agent():\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\t# Init hyperparameters for agent, just like \"self.gamma = opt.gamma, self.lambd = opt.lambd, ...\"\n",
        "\t\tself.__dict__.update(kwargs)\n",
        "\t\tself.tau = 0.005\n",
        "\n",
        "\t\tself.actor = Actor(self.state_dim, self.action_dim, self.net_width, self.max_action).to(self.dvc)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\n",
        "\t\tself.q_critic = Q_Critic(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
        "\t\tself.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=self.c_lr)\n",
        "\t\tself.q_critic_target = copy.deepcopy(self.q_critic)\n",
        "\n",
        "\t\tself.replay_buffer = ReplayBuffer(self.state_dim, self.action_dim, max_size=int(5e5), dvc=self.dvc)\n",
        "\n",
        "\tdef select_action(self, state, deterministic):\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tstate = torch.FloatTensor(state[np.newaxis, :]).to(self.dvc)  # from [x,x,...,x] to [[x,x,...,x]]\n",
        "\t\t\ta = self.actor(state).cpu().numpy()[0] # from [[x,x,...,x]] to [x,x,...,x]\n",
        "\t\t\tif deterministic:\n",
        "\t\t\t\treturn a\n",
        "\t\t\telse:\n",
        "\t\t\t\tnoise = np.random.normal(0, self.max_action * self.noise, size=self.action_dim)\n",
        "\t\t\t\treturn (a + noise).clip(-self.max_action, self.max_action)\n",
        "\n",
        "\tdef train(self):\n",
        "\t\t# Compute the target Q\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\ts, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)\n",
        "\t\t\ttarget_a_next = self.actor_target(s_next)\n",
        "\t\t\ttarget_Q= self.q_critic_target(s_next, target_a_next)\n",
        "\t\t\ttarget_Q = r + (~dw) * self.gamma * target_Q  #dw: die or win\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q = self.q_critic(s, a)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tq_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "\t\t# Optimize the q_critic\n",
        "\t\tself.q_critic_optimizer.zero_grad()\n",
        "\t\tq_loss.backward()\n",
        "\t\tself.q_critic_optimizer.step()\n",
        "\n",
        "\t\t# Update the Actor\n",
        "\t\ta_loss = -self.q_critic(s,self.actor(s)).mean()\n",
        "\t\tself.actor_optimizer.zero_grad()\n",
        "\t\ta_loss.backward()\n",
        "\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t# Update the frozen target models\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\tdef save(self,EnvName, timestep):\n",
        "\t\ttorch.save(self.actor.state_dict(), \"./model/{}_actor{}.pth\".format(EnvName,timestep))\n",
        "\t\ttorch.save(self.q_critic.state_dict(), \"./model/{}_q_critic{}.pth\".format(EnvName,timestep))\n",
        "\n",
        "\tdef load(self,EnvName, timestep):\n",
        "\t\tself.actor.load_state_dict(torch.load(\"./model/{}_actor{}.pth\".format(EnvName, timestep), map_location=self.dvc))\n",
        "\t\tself.q_critic.load_state_dict(torch.load(\"./model/{}_q_critic{}.pth\".format(EnvName, timestep), map_location=self.dvc))"
      ],
      "metadata": {
        "id": "PqbZ6PFaJTB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "\tdef __init__(self, state_dim, action_dim, max_size, dvc):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.dvc = dvc\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.s = torch.zeros((max_size, state_dim) ,dtype=torch.float,device=self.dvc)\n",
        "\t\tself.a = torch.zeros((max_size, action_dim) ,dtype=torch.float,device=self.dvc)\n",
        "\t\tself.r = torch.zeros((max_size, 1) ,dtype=torch.float,device=self.dvc)\n",
        "\t\tself.s_next = torch.zeros((max_size, state_dim) ,dtype=torch.float,device=self.dvc)\n",
        "\t\tself.dw = torch.zeros((max_size, 1) ,dtype=torch.bool,device=self.dvc)\n",
        "\n",
        "\tdef add(self, s, a, r, s_next, dw):\n",
        "\t\t#每次只放入一个时刻的数据\n",
        "\t\tself.s[self.ptr] = torch.from_numpy(s).to(self.dvc)\n",
        "\t\tself.a[self.ptr] = torch.from_numpy(a).to(self.dvc) # Note that a is numpy.array\n",
        "\t\tself.r[self.ptr] = r\n",
        "\t\tself.s_next[self.ptr] = torch.from_numpy(s_next).to(self.dvc)\n",
        "\t\tself.dw[self.ptr] = dw\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size #存满了又重头开始存\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = torch.randint(0, self.size, device=self.dvc, size=(batch_size,))\n",
        "\t\treturn self.s[ind], self.a[ind], self.r[ind], self.s_next[ind], self.dw[ind]"
      ],
      "metadata": {
        "id": "677Zp6otJUt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate setting arguments manually (for testing in Colab)\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.dvc = 'cpu'\n",
        "        self.EnvIdex = 0\n",
        "        self.write = False\n",
        "        self.render = False\n",
        "        self.Loadmodel = False\n",
        "        self.ModelIdex = 100\n",
        "        self.seed = 0\n",
        "        self.Max_train_steps = int(5e6)\n",
        "        self.save_interval = int(1e5)\n",
        "        self.eval_interval = int(2e3)\n",
        "        self.gamma = 0.99\n",
        "        self.net_width = 400\n",
        "        self.a_lr = 1e-3\n",
        "        self.c_lr = 1e-3\n",
        "        self.batch_size = 128\n",
        "        self.random_steps = int(5e4)\n",
        "        self.noise = 0.1\n",
        "\n",
        "opt = Args()\n",
        "opt.dvc = torch.device(opt.dvc) # Convert str to torch.device\n",
        "print(opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgYafZslL9Df",
        "outputId": "8d819c7d-2b6b-4490-fba2-13a837519dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Args object at 0x7d2a3057d330>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    EnvName = ['Pendulum-v1','LunarLanderContinuous-v2','Humanoid-v4','HalfCheetah-v4','BipedalWalker-v3','BipedalWalkerHardcore-v3']\n",
        "    BrifEnvName = ['PV1', 'LLdV2', 'Humanv4', 'HCv4','BWv3', 'BWHv3']\n",
        "\n",
        "    # Build Env\n",
        "    env = gym.make(EnvName[opt.EnvIdex], render_mode = \"human\" if opt.render else None)\n",
        "    eval_env = gym.make(EnvName[opt.EnvIdex])\n",
        "    opt.state_dim = env.observation_space.shape[0]\n",
        "    opt.action_dim = env.action_space.shape[0]\n",
        "    opt.max_action = float(env.action_space.high[0])   #remark: action space【-max,max】\n",
        "    print(f'Env:{EnvName[opt.EnvIdex]}  state_dim:{opt.state_dim}  action_dim:{opt.action_dim}  '\n",
        "          f'max_a:{opt.max_action}  min_a:{env.action_space.low[0]}  max_e_steps:{env._max_episode_steps}')\n",
        "\n",
        "    # Seed Everything\n",
        "    env_seed = opt.seed\n",
        "    torch.manual_seed(opt.seed)\n",
        "    torch.cuda.manual_seed(opt.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(\"Random Seed: {}\".format(opt.seed))\n",
        "\n",
        "    # Build SummaryWriter to record training curves\n",
        "    if opt.write:\n",
        "        from torch.utils.tensorboard import SummaryWriter\n",
        "        timenow = str(datetime.now())[0:-10]\n",
        "        timenow = ' ' + timenow[0:13] + '_' + timenow[-2::]\n",
        "        writepath = 'runs/{}'.format(BrifEnvName[opt.EnvIdex]) + timenow\n",
        "        if os.path.exists(writepath): shutil.rmtree(writepath)\n",
        "        writer = SummaryWriter(log_dir=writepath)\n",
        "\n",
        "\n",
        "    # Build DRL model\n",
        "    if not os.path.exists('model'): os.mkdir('model')\n",
        "    agent = DDPG_agent(**vars(opt)) # var: transfer argparse to dictionary\n",
        "    if opt.Loadmodel: agent.load(BrifEnvName[opt.EnvIdex], opt.ModelIdex)\n",
        "\n",
        "    if opt.render:\n",
        "        while True:\n",
        "            score = evaluate_policy(env, agent, turns=1)\n",
        "            print('EnvName:', BrifEnvName[opt.EnvIdex], 'score:', score)\n",
        "    else:\n",
        "        total_steps = 0\n",
        "        while total_steps < opt.Max_train_steps:\n",
        "            s, info = env.reset(seed=env_seed)  # Do not use opt.seed directly, or it can overfit to opt.seed\n",
        "            env_seed += 1\n",
        "            done = False\n",
        "\n",
        "            '''Interact & trian'''\n",
        "            while not done:\n",
        "                if total_steps < opt.random_steps: a = env.action_space.sample()\n",
        "                else: a = agent.select_action(s, deterministic=False)\n",
        "                s_next, r, dw, tr, info = env.step(a) # dw: dead&win; tr: truncated\n",
        "                done = (dw or tr)\n",
        "\n",
        "                agent.replay_buffer.add(s, a, r, s_next, dw)\n",
        "                s = s_next\n",
        "                total_steps += 1\n",
        "\n",
        "                '''train'''\n",
        "                if total_steps >= opt.random_steps:\n",
        "                    agent.train()\n",
        "\n",
        "                '''record & log'''\n",
        "                if total_steps % opt.eval_interval == 0:\n",
        "                    ep_r = evaluate_policy(eval_env, agent, turns=3)\n",
        "                    if opt.write: writer.add_scalar('ep_r', ep_r, global_step=total_steps)\n",
        "                    print(f'EnvName:{BrifEnvName[opt.EnvIdex]}, Steps: {int(total_steps/1000)}k, Episode Reward:{ep_r}')\n",
        "\n",
        "                '''save model'''\n",
        "                if total_steps % opt.save_interval == 0:\n",
        "                    agent.save(BrifEnvName[opt.EnvIdex], int(total_steps/1000))\n",
        "        env.close()\n",
        "        eval_env.close()"
      ],
      "metadata": {
        "id": "uH4qlqmkL9ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8KCUpC8MBdt",
        "outputId": "2f8e6f94-49ca-483a-9a36-fef12e61ec77"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Env:Pendulum-v1  state_dim:3  action_dim:1  max_a:2.0  min_a:-2.0  max_e_steps:200\n",
            "Random Seed: 0\n",
            "EnvName:PV1, Steps: 2k, Episode Reward:-1218\n",
            "EnvName:PV1, Steps: 4k, Episode Reward:-1251\n",
            "EnvName:PV1, Steps: 6k, Episode Reward:-1523\n",
            "EnvName:PV1, Steps: 8k, Episode Reward:-1177\n",
            "EnvName:PV1, Steps: 10k, Episode Reward:-1319\n",
            "EnvName:PV1, Steps: 12k, Episode Reward:-1246\n",
            "EnvName:PV1, Steps: 14k, Episode Reward:-1161\n",
            "EnvName:PV1, Steps: 16k, Episode Reward:-1265\n",
            "EnvName:PV1, Steps: 18k, Episode Reward:-1283\n",
            "EnvName:PV1, Steps: 20k, Episode Reward:-1111\n",
            "EnvName:PV1, Steps: 22k, Episode Reward:-1224\n",
            "EnvName:PV1, Steps: 24k, Episode Reward:-1332\n",
            "EnvName:PV1, Steps: 26k, Episode Reward:-1400\n",
            "EnvName:PV1, Steps: 28k, Episode Reward:-1249\n",
            "EnvName:PV1, Steps: 30k, Episode Reward:-1370\n",
            "EnvName:PV1, Steps: 32k, Episode Reward:-1334\n",
            "EnvName:PV1, Steps: 34k, Episode Reward:-1157\n",
            "EnvName:PV1, Steps: 36k, Episode Reward:-1146\n",
            "EnvName:PV1, Steps: 38k, Episode Reward:-1223\n",
            "EnvName:PV1, Steps: 40k, Episode Reward:-1116\n",
            "EnvName:PV1, Steps: 42k, Episode Reward:-1394\n",
            "EnvName:PV1, Steps: 44k, Episode Reward:-1319\n",
            "EnvName:PV1, Steps: 46k, Episode Reward:-1256\n",
            "EnvName:PV1, Steps: 48k, Episode Reward:-1041\n",
            "EnvName:PV1, Steps: 50k, Episode Reward:-1706\n",
            "EnvName:PV1, Steps: 52k, Episode Reward:-541\n",
            "EnvName:PV1, Steps: 54k, Episode Reward:-231\n",
            "EnvName:PV1, Steps: 56k, Episode Reward:-83\n",
            "EnvName:PV1, Steps: 58k, Episode Reward:-194\n",
            "EnvName:PV1, Steps: 60k, Episode Reward:-121\n",
            "EnvName:PV1, Steps: 62k, Episode Reward:-118\n",
            "EnvName:PV1, Steps: 64k, Episode Reward:-116\n",
            "EnvName:PV1, Steps: 66k, Episode Reward:-80\n",
            "EnvName:PV1, Steps: 68k, Episode Reward:-118\n",
            "EnvName:PV1, Steps: 70k, Episode Reward:-117\n",
            "EnvName:PV1, Steps: 72k, Episode Reward:-119\n",
            "EnvName:PV1, Steps: 74k, Episode Reward:-152\n",
            "EnvName:PV1, Steps: 76k, Episode Reward:-120\n",
            "EnvName:PV1, Steps: 78k, Episode Reward:-161\n",
            "EnvName:PV1, Steps: 80k, Episode Reward:-75\n",
            "EnvName:PV1, Steps: 82k, Episode Reward:-198\n",
            "EnvName:PV1, Steps: 84k, Episode Reward:-119\n",
            "EnvName:PV1, Steps: 86k, Episode Reward:-124\n",
            "EnvName:PV1, Steps: 88k, Episode Reward:-124\n",
            "EnvName:PV1, Steps: 90k, Episode Reward:-242\n",
            "EnvName:PV1, Steps: 92k, Episode Reward:-170\n",
            "EnvName:PV1, Steps: 94k, Episode Reward:-187\n",
            "EnvName:PV1, Steps: 96k, Episode Reward:-118\n",
            "EnvName:PV1, Steps: 98k, Episode Reward:-42\n",
            "EnvName:PV1, Steps: 100k, Episode Reward:-122\n",
            "EnvName:PV1, Steps: 102k, Episode Reward:-79\n",
            "EnvName:PV1, Steps: 104k, Episode Reward:-85\n",
            "EnvName:PV1, Steps: 106k, Episode Reward:-123\n",
            "EnvName:PV1, Steps: 108k, Episode Reward:-171\n",
            "EnvName:PV1, Steps: 110k, Episode Reward:-160\n",
            "EnvName:PV1, Steps: 112k, Episode Reward:-204\n",
            "EnvName:PV1, Steps: 114k, Episode Reward:-231\n",
            "EnvName:PV1, Steps: 116k, Episode Reward:-140\n",
            "EnvName:PV1, Steps: 118k, Episode Reward:-127\n",
            "EnvName:PV1, Steps: 120k, Episode Reward:-123\n",
            "EnvName:PV1, Steps: 122k, Episode Reward:-151\n",
            "EnvName:PV1, Steps: 124k, Episode Reward:-122\n",
            "EnvName:PV1, Steps: 126k, Episode Reward:-81\n",
            "EnvName:PV1, Steps: 128k, Episode Reward:-125\n",
            "EnvName:PV1, Steps: 130k, Episode Reward:-192\n",
            "EnvName:PV1, Steps: 132k, Episode Reward:-122\n",
            "EnvName:PV1, Steps: 134k, Episode Reward:-80\n",
            "EnvName:PV1, Steps: 136k, Episode Reward:-120\n",
            "EnvName:PV1, Steps: 138k, Episode Reward:-121\n",
            "EnvName:PV1, Steps: 140k, Episode Reward:-157\n",
            "EnvName:PV1, Steps: 142k, Episode Reward:-157\n",
            "EnvName:PV1, Steps: 144k, Episode Reward:-156\n",
            "EnvName:PV1, Steps: 146k, Episode Reward:-151\n",
            "EnvName:PV1, Steps: 148k, Episode Reward:-159\n",
            "EnvName:PV1, Steps: 150k, Episode Reward:-120\n",
            "EnvName:PV1, Steps: 152k, Episode Reward:-117\n",
            "EnvName:PV1, Steps: 154k, Episode Reward:-159\n",
            "EnvName:PV1, Steps: 156k, Episode Reward:-84\n",
            "EnvName:PV1, Steps: 158k, Episode Reward:-125\n"
          ]
        }
      ]
    }
  ]
}